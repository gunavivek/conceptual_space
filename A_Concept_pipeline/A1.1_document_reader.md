# A1.1_document_reader.py - Architecture Specification

## Component Overview

**Script Name**: `A1.1_document_reader.py`  
**Pipeline Stage**: A-Pipeline Foundation (Document Space Entry Point)  
**Primary Function**: PURE document content extraction from Parquet datasets with architectural purity  
**Architecture Role**: Clean document processing for tri-semantic conceptual space (NO question/answer contamination)  

## Functional Specification

### Core Purpose
Extracts PURE document content from FinQA/TatQA Parquet datasets, eliminating questions, answers, and evaluation data to prevent architectural contamination. Converts clean document content into unified JSON format for unbiased downstream A-Pipeline processing. Serves as the architecturally pure entry point for the Document Space in the tri-semantic architecture.

### Input Specifications

#### Primary Input
- **Format**: Apache Parquet files (.parquet)
- **Default Location**: `A_Concept_pipeline/data/test_mode_5_records.parquet`
- **Content Type**: FinQA/TatQA financial question-answering datasets
- **Extracted Fields (ARCHITECTURAL PURITY)**:
  - `text`: Document content ONLY (financial reports, statements)
  - `dataset_name`: Source dataset identifier for audit trail
- **Eliminated Fields (Prevents Data Leakage)**:
  - ❌ `question`: Eliminated to prevent Q&A contamination
  - ❌ `response`: Eliminated to prevent answer bias
  - ❌ Evaluation scores: Eliminated to prevent supervised learning bias

#### Fallback Input
- **Alternative Location**: `A_Concept_pipeline/data/` (any .parquet file)
- **Error Handling**: Raises FileNotFoundError if no valid Parquet files found

### Output Specifications

#### Primary Output
- **Format**: JSON file with UTF-8 encoding
- **Location**: `A_Concept_pipeline/outputs/A1.1_raw_documents.json`
- **Structure**:
```json
{
  "documents": [
    {
      "doc_id": "string",
      "text": "string",
      "metadata": {
        "source": "string",
        "index": "integer",
        "loaded_at": "ISO-8601 timestamp",
        "dataset_name": "string"
      }
    }
  ],
  "count": "integer",
  "source_file": "string",
  "processing_timestamp": "ISO-8601 timestamp"
}
```

## Technical Implementation

### Dependencies
```python
import pandas as pd
import json
import os
from pathlib import Path
from datetime import datetime
```

### Key Functions

#### `load_parquet_data(file_path)`
- **Purpose**: Load and validate Parquet file data
- **Input**: Path to Parquet file
- **Output**: Pandas DataFrame
- **Error Handling**: File existence validation, format verification

#### `process_documents(df, source_file)`
- **Purpose**: Convert DataFrame rows to architecturally pure document objects
- **Processing**:
  - Extracts ONLY document text content (no questions/answers)
  - Generates unique document IDs from dataset and indices
  - Handles missing values safely with pandas.notna()
  - ELIMINATES all evaluation scores and Q&A data
  - Adds minimal, clean metadata tracking for audit trail

#### `save_documents(data, output_path)`
- **Purpose**: Save processed documents to JSON format
- **Features**:
  - UTF-8 encoding with `ensure_ascii=False`
  - Pretty-printed JSON with 2-space indentation
  - Automatic directory creation
  - Comprehensive error handling

### Document ID Generation
```python
doc_id = f"{dataset_name}_{idx}" if dataset_name else f"doc_{idx}"
```

### Document Content Extraction
```python
# ARCHITECTURAL PURITY: Extract only document text
combined_text = extract_document_text_only(row)  # NO questions or answers
```

## Architecture Integration

### Tri-Semantic Architecture Role
- **Document Space Entry**: First component in A-Pipeline document processing
- **Data Standardization**: Normalizes diverse Parquet formats into unified JSON schema
- **Metadata Enrichment**: Adds processing timestamps and source tracking for audit trails

### Pipeline Flow Position
```
Input Data Sources → A1.1_document_reader.py → A1.2_document_domain_detector.py → A2.1_preprocess_document_analysis.py
```

### Integration Points
- **Downstream Dependency**: A1.2 Document Domain Detector reads A1.1 output
- **Input Flexibility**: Handles various FinQA dataset formats and structures
- **Non-Destructive**: Preserves all original data while adding processing metadata

## Performance Characteristics

### Capacity
- **Test Dataset**: 5 documents (test mode)
- **Scalability**: Designed for larger datasets through pandas vectorization
- **Memory Usage**: Loads entire Parquet file into memory (suitable for typical document sets)

### Processing Speed
- **Typical Runtime**: <1 second for test dataset
- **Bottlenecks**: File I/O operations (Parquet read, JSON write)
- **Optimization**: Uses pandas for efficient data manipulation

## Error Handling & Validation

### Input Validation
- **File Existence**: Validates Parquet file presence before processing
- **Format Verification**: Ensures DataFrame conversion success
- **Required Fields**: Gracefully handles missing optional fields

### Output Validation
- **Directory Creation**: Ensures output directory exists
- **JSON Serialization**: Handles Unicode characters properly
- **Atomic Writes**: Prevents partial file corruption

### Exception Handling
```python
try:
    # Core processing
except FileNotFoundError:
    print(f"Error: Data file not found at {file_path}")
    raise
except Exception as e:
    print(f"Error processing documents: {str(e)}")
    raise
```

## Quality Assurance

### Data Integrity
- **Source Preservation**: Original text and metadata maintained
- **Timestamp Tracking**: Processing time recorded for audit trails
- **Index Mapping**: Original DataFrame indices preserved in metadata

### Output Verification
- **Document Count Validation**: Verifies all input records processed
- **JSON Schema Consistency**: Standardized output format across all documents
- **Encoding Safety**: UTF-8 encoding prevents character corruption

## Future Enhancement Opportunities

### Potential Improvements
1. **Batch Processing**: Support for multiple Parquet files in single run
2. **Format Detection**: Automatic detection of dataset schema variations
3. **Compression**: Optional output compression for large document sets
4. **Validation Schema**: JSON Schema validation for output format verification
5. **Streaming Processing**: Memory-efficient processing for very large datasets

### Architecture Evolution
- **R-Pipeline Integration**: Could leverage BIZBOK ontology for early concept detection
- **Preprocessing Integration**: Could include basic text cleaning at ingestion stage
- **Multi-Format Support**: Extension to handle PDF, TXT, CSV document sources

## Architectural Purity Implementation

### Data Leakage Elimination
- **Problem Identified**: Previous version imported questions, answers, and evaluation scores
- **Risk**: A-Pipeline could be biased by question-answer patterns rather than discovering inherent document concepts  
- **Solution**: Eliminated all non-document fields to ensure pure document analysis
- **Impact**: A-Pipeline now analyzes documents independently, suitable for any document corpus

### Production Readiness Benefits
- **Real-World Deployment**: Can process any document collection (reports, contracts, news) without Q&A data
- **Unbiased Concept Discovery**: Extracts concepts based solely on document content  
- **Architecture Compliance**: Maintains true separation between Document Space (A-Pipeline) and Question Space (B-Pipeline)
- **Scientific Integrity**: Enables genuine unsupervised document analysis without answer contamination

## Compliance Status

### Architecture Alignment  
- ✅ **Functional Specification**: Matches "Load & Parse Documents" architectural purpose
- ✅ **Pipeline Position**: Correctly positioned as A-Pipeline entry point  
- ✅ **Input/Output Contract**: Fulfills "Raw Documents → Parsed Docs" specification
- ✅ **Naming Convention**: Uses A1.1_document_reader.py (actual implementation)
- ✅ **Architectural Purity**: ELIMINATES data leakage and Q&A contamination
- ✅ **No Cross-Pipeline Dependencies**: Correctly shows no R4X/I-InterSpace integration at this stage

### System Integration
- ✅ **Downstream Compatibility**: Output format compatible with A1.2 requirements
- ✅ **Error Resilience**: Robust error handling prevents pipeline failures
- ✅ **Data Preservation**: Non-destructive processing maintains data fidelity
- ✅ **Audit Trail**: Comprehensive metadata supports system debugging and validation

---

**Document Version**: 2.0 (ARCHITECTURAL PURITY)  
**Last Updated**: 2025-09-01  
**Architecture Status**: ✅ SYNCHRONIZED with A1.1_document_reader.py  
**Data Leakage**: ✅ ELIMINATED - No questions, answers, or evaluation contamination  
**Production Readiness**: ✅ READY for real-world document processing  
**Script Location**: `A_Concept_pipeline/scripts/A1.1_document_reader.py`  
**Output Location**: `A_Concept_pipeline/outputs/A1.1_raw_documents.json`