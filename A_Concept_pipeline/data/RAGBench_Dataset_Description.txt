RAGBench Dataset Details
RAGBench is sourced from publicly released acadmic and industry datasets. As far as we know, none
of the component datasets contain personally identifiable information or offensive content.

PubMedQA: PubMedQA is a collection of PubMed research abstracts with corresponding
yes/no/maybe questions paired with each abstract. The original dataset comprises 3 subsets: PQA-L,
PQA-U, and PQA-A, with 1k, 60k, and 210k abstracts, respectively. For all subsets, the question
is derived from the title of the PubMed article using rule-based heuristics. Long answers are
automatically derived from the last sentence of the abstract for PQA-L and PQA-U, and QA-L
answers are further reviewed by expert annotators and annotated as yes/no/maybe. PQA-A comprises
exclusively automatically generated questions and short answers.
For RAGBench we utilize the PQA-U subset and re-frame it from QA into a RAG task. To simulate
RAG, we leverage already segmented PQA-U abstracts context chunks and we encode them into a
vector DB with OpenAI embeddings. The size of the resulting DB is 200k. We retrieve 4 chunks for
each PQA-U question using FAISS with eucledian distance as the similarity function. We ignore the
responses and labels in the original dataset and generate new responses with an LLM.

CovidQA-RAG: CovidQA-RAG is a combination of 2k expert-annotated questions sourced from
COVID-QA [26] and a vector database of 250,000 100-word passages built by Siriwardhana et al.
[34]. Both questions and answers are sourced from CORD-19 [37] collection of research articles
about COVID-19.
We embed the questions and database passages with OpenAI embeddings and retrieve up to N
passages for each COVID-QA question from the vector database using FAISS with eucledian distance
as the similarity function and max_distance=0.25. We generate responses for each resulting RAG
(context, question) instance with an LLM.

HotpotQA: HotpotQA comprises 113K crowd-sourced question-answer pairs sourced from
Wikipedia. Each pair is associated with a set of related context passages from one or multiple
Wikipedia pages. The dataset is constructed in a way that requires multi-hop reasoning over multiple
context documents to arrive at the answer, which renders it a valuable candidate for our benchmark.
We sample data from the dev-distractor split, which contains up to 8 distractor context documents
per sample. We downsample the context documents to 4 per example, making sure to include the
document containing the response. We treat the context passages in HotpotQA as RAG context
documents, and generate responses for each (context, question) instance with an LLM.

MS Marco: MS Marco is an open-domain question answering dataset sourced from Bing
search engine user query logs. Each question is associated with 10 context passages retrieved via
Bing web search. Human annotators compose a response based on the provided context documents,
and label the documents utilized in the response as relevant. We sample data from the original version
of the dataset, comprising 80k train, 10k validation, and 10k test samples. As with other datasets, we
ignore the human annotated answers and generate responses with an LLM in RAG setting.

CUAD: CUAD is a collection of commercial legal contracts with expert annotated questions
and responses. The contracts are sourced from a public legal contract library(EDGAR) and range
from 1-100 pages in length. Experts in the legal domain compose multiple questions per contract
and label the relevant parts of the contract that are useful for answering the questions. There are
21k questions pertaining to 510 documents in total. The questions are very specific to each contract,
thus we don’t perform additional retrieval over the contract corpus, and form RAG examples with 1
context contract each for our benchmark. Due to high anntoation costs associated with long-context
RAG, we sample 5 question per doc. As with other datasets, we generate responses with an LLM in
RAG setting.

DelucionQA: DelucionQA is a domain-specific RAG dataset leveraging Jeep’s 2023 Gladiator
model manual as the source of knowledge. The questions and answers are automatically generated by
large language models. RAG context passages are retrieved from the Jeep car manual via both sparse
and dense retrieval methods to add variance in the sample distribution. Further, MTurk workers
annotate whether or not responses are supported by the context.
Upon closer inspection, we found only 1 relevant passage associated with each question in the
DelucionQA dataset. To make the dataset more challenging for RAGBench, we build a vector
database from the 1,046 context passages in DelucionQA and and retrieve up to 3 context documents
per question from it. We use text-embedding-ada-002 embeddings from OpenAI to build the
database. There are 913 unique questions in DelucionQA. For each resulting (context, question)
sample, we generate responses with an LLM.

EManual: EManual is a question answer dataset comprising consumer electronic device
manuals and realistic questions about them composed by human annotators. The subset made
available at the time of writing amounts to 659 unique questions about the Samsung Smart TV/remote
and the accompanying user manual, segmented into 261 chunks. To form a RAG dataset, we embed
the manual segments into a vector database with OpenAI embedding and retrieve up to 3 context
documents per question from it. For each resulting (context, question) sample, we generate responses
with an LLM.

TechQA: TechQA is a collection of real-world user questions posted on IBMDeveloper and
DeveloperWorks forums, along with 50 technical support documents relating to each question. The
documents are sourced from database of 800k technical documents that support accepted answers
on the tech forums. The authors release 1.4k questions, split between train, validation, and test sets.
The data are curated such that fractions on the each split unanswerable given the information in the
linked documents, which makes it a good candidate for RAGBench. To reduce annotation costs,
we sub-sample the data down to 10 documents per question, making sure to include the document
containing the answer, when applicable. We use the provided splits with (context document, question)
examples and generate responses for each with an LLM.

FinQA: FinQA is a QA dataset of financial report passages and associated questions. Questions
are curated such that numerical reasoning over multiple unstructured and tabular inputs is required to
arrive at the answer. FinQA totals 8,281 financial QA pairs, split between train, validation, and test
splits. We retain the original splits and generate 2 LLM responses per each context-query example in
FinQA.

TAT-QA: TAT-QA is another financial QA dataset that requires numerical reasoning over
tables and text. The data are sourced from 500 financial reports released on https://www.
annualreports.com/. Expert annotators with background in finance annotate question-answer
pairs based on the available documents. We leverage the full dataset (13k train, 1.6k validation and
test) but generate new responses with LLMs for RAGBench.

HAGRID: HAGRID is a QA dataset built on top of MIRACL [45], a multi-lingual information retrieval dataset. 
HAGRID passes questions and relevant context documents from MIRACLE through
an LLM to produce a response for each example in the dataset. Annotors then rate the response
on informativeness and attribution dimensions. The original context documents are sourced from
Wikipedia and associated questions are generated by expert annotators. Since HAGRID already
contains LLM-generated responses, we directly use them and don’t generate additional responses for
RAGBench.

ExpertQA: ExpertQA is a collection of curated questions from domain-experts in various
fields of sicence, arts, and law. The dataset also contains expert curated passsages relevant to each
question, alongside LLM-generated responses. As with HAGRID, we leverage the LLM-generated
responses in ExpertQA directly for our RAG dataset.